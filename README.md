# ğŸ§  Modular RAG PDF Chatbot with FastAPI, ChromaDB & Streamlit


This project is a modular **Retrieval-Augmented Generation (RAG)** application that allows users to upload PDF documents and chat with an AI assistant that answers queries based on the document content. It features a microservice architecture with a decoupled **FastAPI backend** and **Streamlit frontend**, using **ChromaDB** as the vector store and **Groq's LLaMA3 model** as the LLM.

---

## ğŸ“‚ Project Structure

```
ragbot2.0/
â”œâ”€â”€ client/         # Streamlit Frontend
â”‚   |â”€â”€components/
|   |  |â”€â”€chatUI.py
|   |  |â”€â”€history_download.py
|   |  |â”€â”€upload.py
|   |â”€â”€utils/
|   |  |â”€â”€api.py
|   |â”€â”€app.py
|   |â”€â”€config.py
â”œâ”€â”€ server/         # FastAPI Backend
â”‚   â”œâ”€â”€ chroma_store/ ....after run
|   |â”€â”€modules/
â”‚      â”œâ”€â”€ load_vectorestore.py
â”‚      â”œâ”€â”€ llm.py
â”‚      â”œâ”€â”€ pdf_handler.py
â”‚      â”œâ”€â”€ query_handlers.py
|   |â”€â”€uploaded_pdfs/ ....after run
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ main.py
â””â”€â”€ README.md
```

---

## âœ¨ Features

- ğŸ“„ Upload and parse PDFs
- ğŸ§  Embed document chunks with HuggingFace embeddings
- ğŸ’‚ï¸ Store embeddings in ChromaDB
- ğŸ’¬ Query documents using LLaMA3 via Groq
- ğŸŒ Microservice architecture (Streamlit client + FastAPI server)

---

## ğŸ“ How RAG Works

Retrieval-Augmented Generation (RAG) enhances LLMs by injecting external knowledge. Instead of relying solely on pre-trained data, the model retrieves relevant information from a vector database (like ChromaDB) and uses it to generate accurate, context-aware responses.


## ğŸš§ TODO

- [ ] Add authentication for endpoints
- [ ] Dockerize the project
- [ ] Add support for more file types

